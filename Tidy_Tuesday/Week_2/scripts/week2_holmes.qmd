---
title: "Tidy Tuesday-Week 2: Sherlock Holmes"
author: "Sk Abidur Rahman"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
format:
  html:
    toc: true
    theme: flatly
execute:
  warning: false
  message: false
---


## Overview
This **Tidy Tuesday** explores *The Complete Sherlock Holmes* lines.

**What we are doing:**
1. Split the text into **sentences** using the `tidytext` package.
2. Guess the **speaker** (`Holmes`, `Watson`, `Other`) based on simple text patterns.
3. Compare **sentence length** and **sentiment** across these speakers and over the course of the books.

```{r}
# packages for data wrangling and text analysis
library(tidyverse) # for data cleaning and plotting
library(tidytext)#breaking text into words/sentences (tokenizing)
library(here) # Helps find files and folders easily across different computers
library(gt) # summary tables
```


###Load data
```{r}
# web address for the raw data file from the TidyTuesday repository
url <- "https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-11-18/holmes.csv"

# Read the data directly from the URL
holmes <- readr::read_csv(url, show_col_types = FALSE) |>
  mutate(text = coalesce(text, ""))# Replace any missing text values (NA)

# check
glimpse(holmes)
```


# Convert to sentences while preserving book & line order
```{r}
sentences <- holmes |>
  # Create a line number for each book to keep track of order
mutate(line_num = row_number(), .by = book) |>
# tidytext::unnest_tokens to break the main text into individual sentence rows.
  # token=sentences finding sentence breaks.
  # to_lower = FALSE keeps capitalization
  unnest_tokens(sentence, text, token = "sentences", to_lower = FALSE, drop = TRUE) |>
  # Remove empty strings
filter(nchar(sentence) > 0) |>
  # Create a sentence ID within each book to maintain order
mutate(sent_id = row_number(), .by = book)
```


#Heuristic Speaker Tagging
```{r}
# Define the patterns case-insensitive
# \b ensures we match whole words e.g., 'said' not 'inside'
holmes_patterns <- regex("\b(Holmes said|said Holmes)\b", ignore_case = TRUE)
watson_patterns <- regex("\b(I said|said I|Watson said|said Watson)\b", ignore_case = TRUE)

# create a new speaker column
sentences <- sentences |>
mutate(
speaker = case_when(
str_detect(sentence, holmes_patterns)  ~ "Holmes",
str_detect(sentence, watson_patterns)  ~ "Watson",
TRUE                                   ~ "Other"
),
# Convert the speaker column to a Factor and set a specific order
speaker = factor(speaker, levels = c("Holmes","Watson","Other"))
)
```


##Sentence length 
```{r}
# count words per sentence
# break each sentence row into individual word rows
sent_word_counts <- sentences |>
unnest_tokens(word, sentence, token = "words", to_lower = FALSE) |>
# Group by book, sentence ID, and speaker, then count the words in each sentence
  count(book, sent_id, speaker, name = "n_words")

# summary stat by speaker
len_by_speaker <- sent_word_counts |>
group_by(speaker) |>
summarise(
mean_len = mean(n_words, na.rm = TRUE),#avg mean sent length
med_len  = median(n_words, na.rm = TRUE), #median sent leng
n_sent   = dplyr::n(), #total no, sent count
.groups = "drop"
)

# check
len_by_speaker
```


###Plot:sentence length distribution (Violin/Boxplot)
```{r}
#Sentence length by speaker (word count)
p_len <- sent_word_counts |>
#Filter out any rows where the speaker couldn't be tagged (NA)
filter(!is.na(speaker)) |>
  #plot: X-axis is speaker, Y-axis is word count
ggplot(aes(x = speaker, y = n_words, fill = speaker)) +
# Add a violin plot to show the overall shape of the distribution
  geom_violin(alpha = 0.5, color = NA, width = 0.9, trim = TRUE) +
# Add a boxplot inside to show the median and quartiles 
  geom_boxplot(width = 0.2, outlier.shape = NA, alpha = 0.9) +
   # color palette and hode legend
scale_fill_brewer(palette = "Set2", guide = "none") +
# add title and labels
  labs(
title = "Sherlock Holmes: Sentence Length by Speaker",
subtitle = "Heuristic speaker tagging; word count per sentence",
x = "Speaker", y = "Words per sentence"
) +
# add theme
  theme_minimal(base_size = 12) +
theme(panel.grid.minor = element_blank())

# check
p_len

# save plot
ggsave(
filename = here("Tidy_Tuesday","Week_2","output","sentence_length_by_speaker.png"),
plot = p_len, width = 9, height = 5, dpi = 320
)
```


###Sentiment (bing) over narrative position (per book)

```{r}
#| label: sentiment
bing <- get_sentiments("bing") # positive / negative lexicon

sentiment_by_sent <- holmes |>
mutate(line_num = row_number(), .by = book) |>
unnest_tokens(word, text, token = "words") |>
inner_join(bing, by = "word") |>
mutate(score = if_else(sentiment == "positive", 1L, -1L)) |>
group_by(book, line_block = (row_number() %/% 200)) |>
summarise(net_sent = sum(score), .groups = "drop")  # coarse rolling-ish blocks

# Show top 8 books by variability in sentiment

top_books <- sentiment_by_sent |>
group_by(book) |>
summarise(sd_sent = sd(net_sent), .groups = "drop") |>
slice_max(sd_sent, n = 8) |>
pull(book)

p_sent <- sentiment_by_sent |>
filter(book %in% top_books) |>
ggplot(aes(x = line_block, y = net_sent, group = book)) +
geom_line() +
facet_wrap(vars(book), scales = "free_y") +
labs(
title = "Net sentiment across the narrative (blocks of ~200 words)",
x = "Narrative block index", y = "Net sentiment (bing)"
) +
theme_minimal(base_size = 12)

p_sent

ggsave(
filename = here("Tidy_Tuesday","Week_2","output","sentiment_over_time.png"),
plot = p_sent, width = 10, height = 6, dpi = 320
)
```

#Table: mean sentence length by speaker
```{r}
#| label: table
len_by_speaker |>
arrange(desc(mean_len)) |>
mutate(
mean_len = round(mean_len, 1),
med_len  = round(med_len, 1)
) |>
gt() |>
tab_header(
title = "Sherlock Holmes â€” Sentence Length by Speaker",
subtitle = "Heuristic speaker tags"
)

